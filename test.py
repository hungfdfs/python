import time
import json
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

class FacebookCommentCrawler:
    def __init__(self, chrome_driver_path, cookies_file):
        self.chrome_driver_path = chrome_driver_path
        self.cookies_file = cookies_file
        self.driver = None
        self.comments_data = []

    def setup_driver(self):
        """Kh·ªüi ƒë·ªông tr√¨nh duy·ªát Selenium"""
        chrome_options = Options()
        chrome_options.add_argument("--disable-notifications")
        chrome_options.add_argument("--start-maximized")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")

        service = Service(self.chrome_driver_path)
        self.driver = webdriver.Chrome(service=service, options=chrome_options)

    def load_cookies(self):
        """ƒêƒÉng nh·∫≠p v√†o Facebook b·∫±ng cookies"""
        self.driver.get("https://www.facebook.com/")
        time.sleep(3)
        with open(self.cookies_file, "r") as f:
            cookies = json.load(f)
            for cookie in cookies:
                self.driver.add_cookie(cookie)
        self.driver.refresh()
        time.sleep(5)
        print("‚úÖ ƒêƒÉng nh·∫≠p th√†nh c√¥ng!")

    def get_all_post_urls(self, page_id):
        """L·∫•y danh s√°ch b√†i vi·∫øt t·ª´ fanpage b·∫±ng c√°ch cu·ªôn trang cho ƒë·∫øn khi kh√¥ng c√≤n b√†i m·ªõi"""
        post_urls = set()
        page_url = f"https://www.facebook.com/{page_id}"
        self.driver.get(page_url)
        time.sleep(5)

        last_height = self.driver.execute_script("return document.body.scrollHeight")
        while True:
            posts = self.driver.find_elements(By.XPATH, "//a[contains(@href, '/posts/')]")
            for post in posts:
                try:
                    url = post.get_attribute("href").split("?")[0]
                    post_urls.add(url)
                except Exception:
                    continue

            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                # N·∫øu kh√¥ng thay ƒë·ªïi chi·ªÅu cao, c√≥ th·ªÉ ƒë√£ cu·ªôn h·∫øt
                break
            last_height = new_height
            print(f"üîé ƒê√£ l·∫•y ƒë∆∞·ª£c {len(post_urls)} b√†i vi·∫øt...")

        print(f"üîé T·ªïng c·ªông l·∫•y ƒë∆∞·ª£c {len(post_urls)} b√†i vi·∫øt")
        return list(post_urls)

    def expand_all_comments(self):
        """Nh·∫•n n√∫t 'Xem th√™m b√¨nh lu·∫≠n' ƒë·ªÉ t·∫£i to√†n b·ªô b√¨nh lu·∫≠n"""
        while True:
            try:
                more_comments_button = self.driver.find_element(By.XPATH, "//span[contains(text(), 'Xem th√™m b√¨nh lu·∫≠n')]")
                self.driver.execute_script("arguments[0].click();", more_comments_button)
                time.sleep(3)
            except Exception:
                break

    def get_comments_from_post(self, post_url):
        """L·∫•y t·∫•t c·∫£ b√¨nh lu·∫≠n t·ª´ b√†i vi·∫øt"""
        self.driver.get(post_url)
        time.sleep(5)

        self.expand_all_comments()  # T·∫£i h·∫øt b√¨nh lu·∫≠n tr∆∞·ªõc khi l·∫•y d·ªØ li·ªáu

        comments_elements = self.driver.find_elements(By.XPATH, "//div[contains(@aria-label, 'B√¨nh lu·∫≠n')]")
        for comment in comments_elements:
            try:
                commenter_element = comment.find_element(By.XPATH, ".//a")
                commenter_name = commenter_element.text
                username = commenter_element.get_attribute("href").split("?")[0]

                comment_text_elements = comment.find_elements(By.XPATH, ".//div[2]//span")
                comment_text = " ".join([c.text for c in comment_text_elements if c.text.strip()])

                try:
                    reactions_element = comment.find_element(By.XPATH, ".//span[contains(@aria-label, 'l∆∞·ª£t th√≠ch')]")
                    reactions_count = int(reactions_element.text) if reactions_element.text.isdigit() else 0
                except Exception:
                    reactions_count = 0

                comment_date = "Kh√¥ng x√°c ƒë·ªãnh"

                self.comments_data.append({
                    'username': username,
                    'commenter_name': commenter_name,
                    'post_url': post_url,
                    'comment_date': comment_date,
                    'comment_text': comment_text,
                    'comment_reactions': reactions_count
                })
            except Exception:
                continue

        print(f"üí¨ Thu th·∫≠p {len(comments_elements)} b√¨nh lu·∫≠n t·ª´ {post_url}")

    def crawl_fanpage(self, page_id):
        """Crawl t·∫•t c·∫£ b√¨nh lu·∫≠n t·ª´ fanpage m√† kh√¥ng gi·ªõi h·∫°n s·ªë b√†i vi·∫øt"""
        self.setup_driver()
        self.load_cookies()

        post_urls = self.get_all_post_urls(page_id)

        for post_url in post_urls:
            self.get_comments_from_post(post_url)
            time.sleep(2)

        output_file = f"facebook_comments_{page_id}.xlsx"
        df = pd.DataFrame(self.comments_data)
        df.to_excel(output_file, index=False)

        print(f"‚úÖ Crawl ho√†n t·∫•t! D·ªØ li·ªáu ƒë√£ l∆∞u v√†o {output_file}")
        self.driver.quit()

if __name__ == "__main__":
    chrome_driver_path = "F:/chromedriver-win64/chromedriver.exe"
    cookies_file = "cookies.json"
    page_id = "bistar.ecopark"

    crawler = FacebookCommentCrawler(chrome_driver_path, cookies_file)
    crawler.crawl_fanpage(page_id)
